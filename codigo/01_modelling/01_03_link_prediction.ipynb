{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/5/5c/Logo_Mondragon_Unibertsitatea.png\" width=\"150\" height=\"100\" float =\"left\">\n",
    "<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAPIAAADQCAMAAAAK0syrAAAAgVBMVEX///8AAAD7+/t6enrV1dWYmJjj4+Po6OjQ0NDx8fGIiIjq6ur39/fv7+/09PTf39+vr69xcXHGxsa5ubmfn59paWmnp6eTk5M8PDxZWVm9vb1eXl5ubm7BwcEzMzOHh4cnJycdHR0TExNPT09FRUUYGBhaWlpEREQ4ODgqKipjY2MMKFclAAAEmElEQVR4nO3ZaXOyPBQGYG52VzbFfQO1tf//Bz5JqArYzvSd8IK29/Whg4ymJyQ5WTAMIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiKiXyXYdB1B2yxg0nUM7VoBWHcdRKumiO2/1cozbIwQQddhtMhH3zDO6DqMFpnYGsYYTtdxtGiKkWHs0Os6jvb4SA1jhGnXcbRoL7q1kcPrOo72JHANka6truNokazsGH8pXacwDeMNftdxtEjOTX3MdIrworipaNowET16g4VWGXvYDUXTCjFBBXIloiHU6yNtm8HwcDC1ynit1Gci7GGvV+PwtXZgMwSYa5YB6D2ylgH40Cyij10jobQkhXa8Lmor1WR/S/+2P9QsvXEBsNQtY4u8/DE+wHI/rzd4uv3oBPprrj5QWoaICjulge2FI93yG9WbN3C+F6M0Q9nbp2vVihRRqn3YNcK97wZ5MxU2jeR/OVC3T6JPL7SXEJmocnGW4p7l8Zm2wS5aA1EDJdULXqj4tLP129zEWV4EC6zqw3ZQydaelVuPfcq03crHqdy6zxp5eBWjCIuBIc9/NPc/K5i23FD4C6wfjlQmyEqfQsBaVKaHvhgGlugkSG63bNUGNrDXi6vOXeOteNwp9GbNHCNR63hzAeaPJ4WV2SDFxRSPuLT78ETuPItnHhxw/GzpQC1cZzjhoBVXjZ8jv270rB8P5SB10tivbhA9bE25DrmyqqtOq3ywJL42NHoZxvdbB/kb1cvEVJkve+qWvbQwFVVvbiz3ROnRffDk7z/7meiUKsByfjd3Kj1PcViqqoq+XTkT7pcX3l6GhR/hXOpUC1nedTcSi4yl/sV+HYpCjo29NVnmQFpuisNPzgWGM8gj3yUsb4fT8BbjWlZwUNqSTHC6/0rU6NbI40g9r6jcSXJ5p7xqG3oDBxd1GTW0+15+iCBqRxff9h/Pes+wSkbmQP5MtYWqgdyD7PrRu5iLiy5qlU9DrNsGxYPIS2Fx7Voi/75Vj09tzEWOQj3FA33T9OcNzO4j3zmK+j4uLb+bo0LZBPtihB6LfBqh+LuyVqtdeM3O4+ppykV2B0O2dyI69lFcDSYH7Mayjtl9PMVH+T08zkQj1fYfbv3+z02BbLtVcTtfDo5vquxAheOLlHtNN+Mivkst3/VLE4yhlmLis53Jpu8VT2xbPAS5Ll0lgRvETo6tav9088Um23Q1Xx/44WTmhPG3pXx9WC/GbjFCrXsWWapuWn//bNYPgMRT+phfW2+ThuWtY7JSfWeaDv5bJZr15Vh2EDvFOLRwiy5RGeXw0Mj1PGMv8tlTv8vD6vFeKBYQoZov3VInGIsuuzyh2kAuXu9VZfY4LydyNIYyZ/qVo225R6w3afiZrV7J40YqVF1apJ7JtPml/TOY1d8xTD7X/fY8izSmiifm11J2/w+8issqCyC1V/3tNqUl8vhS3uf8XtZtaZ98NWP9SmKpFQ6NYbLXP89+GcGHXAeeXm+G1RIET/cWhYiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIiIqFn/AA1nKx7ECwr8AAAAAElFTkSuQmCC\" width=\"200\" height=\"120\" float =\"left\">    \n",
    "\n",
    "\n",
    "---\n",
    "<h3>Beñat Basabe, Jon Jarrín, June Pagaldai, Daniel Puente, Eneko Rentería | <font color='red'> Equipo Rojo<h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction\n",
    "Se generan multiples redes prediciendo los edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d8baeb",
   "metadata": {
    "executionInfo": {
     "elapsed": 328,
     "status": "ok",
     "timestamp": 1639141048326,
     "user": {
      "displayName": "Eneko Renteria Gabantxo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11839893040904061534"
     },
     "user_tz": -60
    },
    "id": "47d8baeb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def guardar_pikle(df, name):\n",
    "    with open(dir_out + str(name) + '.pkl', 'wb') as fp: pickle.dump(df, fp)\n",
    "\n",
    "def abrir_pikle(name):\n",
    "    with open(dir_out + str(name) + '.pkl', 'rb') as fp: df = pickle.load(fp)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d2c59d",
   "metadata": {
    "id": "f8d2c59d"
   },
   "source": [
    "- **Pyspark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb04edd4",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1639141048708,
     "user": {
      "displayName": "Eneko Renteria Gabantxo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11839893040904061534"
     },
     "user_tz": -60
    },
    "id": "bb04edd4"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c037de3",
   "metadata": {
    "id": "4c037de3"
   },
   "source": [
    "- **Strings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac86bb",
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1639141048710,
     "user": {
      "displayName": "Eneko Renteria Gabantxo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11839893040904061534"
     },
     "user_tz": -60
    },
    "id": "1cac86bb"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf59971",
   "metadata": {
    "id": "fdf59971"
   },
   "source": [
    "- **Visualización**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0398c1",
   "metadata": {
    "executionInfo": {
     "elapsed": 1104,
     "status": "ok",
     "timestamp": 1639141049805,
     "user": {
      "displayName": "Eneko Renteria Gabantxo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11839893040904061534"
     },
     "user_tz": -60
    },
    "id": "dc0398c1"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3MzuEph_Z8Cg",
   "metadata": {
    "id": "3MzuEph_Z8Cg"
   },
   "source": [
    "* **Grafos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-lxS1ncRZwDT",
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1639141049807,
     "user": {
      "displayName": "Eneko Renteria Gabantxo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11839893040904061534"
     },
     "user_tz": -60
    },
    "id": "-lxS1ncRZwDT"
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from torch_geometric.utils import to_networkx, from_networkx, train_test_split_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921c8285",
   "metadata": {},
   "source": [
    "- **Otros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09ab5c8",
   "metadata": {
    "id": "b09ab5c8"
   },
   "source": [
    "## Funciones a utilizar\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_in, dir_out = \"../../datos/datos_originales/\", \"../../datos/datos_desarrollo/\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"how to read csv file\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae999b7",
   "metadata": {
    "id": "8ae999b7"
   },
   "source": [
    "- Función para **dar nombre** a la variable **family_id** mediante la variable title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b543aa1",
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1639141056405,
     "user": {
      "displayName": "Eneko Renteria Gabantxo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11839893040904061534"
     },
     "user_tz": -60
    },
    "id": "2b543aa1"
   },
   "outputs": [],
   "source": [
    "def get_family_name(id_):\n",
    "    \n",
    "    filtered = product.filter(product.family_id == id_)\n",
    "    string = ' '.join([x['title'] for x in filtered.select('title').collect()]).lower()\n",
    "    \n",
    "    return id_, Counter(string.split()).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1fe385",
   "metadata": {
    "id": "ca1fe385"
   },
   "source": [
    "- Función para crear los **sufijos** y facilitar el *join*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cdbe9c",
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1639141056407,
     "user": {
      "displayName": "Eneko Renteria Gabantxo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11839893040904061534"
     },
     "user_tz": -60
    },
    "id": "60cdbe9c"
   },
   "outputs": [],
   "source": [
    "def rename_columns(df, suffix):\n",
    "    ''' Renombra las columnas de un dataframe.'''\n",
    "    for names in df.schema.names: df = df.withColumnRenamed(names,names + suffix)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20lh66ZdaJOU",
   "metadata": {
    "id": "20lh66ZdaJOU"
   },
   "source": [
    "* Función para conseguir valores únicos de una variable en *pyspark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3KNGJEaGCt",
   "metadata": {
    "executionInfo": {
     "elapsed": 63,
     "status": "ok",
     "timestamp": 1639141056408,
     "user": {
      "displayName": "Eneko Renteria Gabantxo",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "11839893040904061534"
     },
     "user_tz": -60
    },
    "id": "7e3KNGJEaGCt"
   },
   "outputs": [],
   "source": [
    "def get_value(dffs, variable):\n",
    "    return list(dffs.select(dffs[variable]).distinct().toPandas()[variable])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G7, G8, G9 = abrir_pikle('G7'), abrir_pikle('G8'), abrir_pikle('G9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G7, G8, G9 = abrir_pikle('G7'), abrir_pikle('G8'), abrir_pikle('G9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.1.2'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from node2vec import Node2Vec\n",
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.datasets import Planetoid\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se genera el modelo para hacer el embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2vec = Node2Vec(G7, dimensions=64, walk_length=10, num_walks=10, workers=4)\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "t10cu_emb = { n: list(map(float,model.wv.get_vector(n))) for n in G7.nodes()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se incluye el embedding en el nodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, v  in t10cu_emb.items(): G7.nodes[x]['x'] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Embedding manual, es simplemente un one hot enocodding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in ['color','size','level']:\n",
    "    attr = [G7.nodes[x][v] for x in G7.nodes()]\n",
    "\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(attr)\n",
    "\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "    guardar_pikle(onehot_encoded, 'onehot_encoded_' + v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color, size, level = abrir_pikle('onehot_encoded_color'), abrir_pikle('onehot_encoded_size'), abrir_pikle('onehot_encoded_level')\n",
    "encoding = np.concatenate((color, size, level), axis=1)\n",
    "\n",
    "for k, v in enumerate(encoding.tolist()): G7.nodes[k]['x_encoded'] = v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Otro ejemplo de embedding que **no va tan bien**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AnonymousWalkKernel import AnonymousWalks\n",
    "\n",
    "aw = AnonymousWalks(G7)\n",
    "length = 3 # length of the walks\n",
    "\n",
    "aw._all_paths(steps=length)\n",
    "print('All possible anonymous walks of length {} (a.k.a embedding size)'.format(length))\n",
    "print(aw.paths[length])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se pasa a formato de pytorch y se divide en train test los edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 232104], color=[2079], size=[2079], level=[2079], x_encoded=[2079, 50], weight=[232104], num_nodes=2079)\n",
      "Data(color=[2079], size=[2079], level=[2079], x_encoded=[2079, 50], weight=[232104], num_nodes=2079, val_pos_edge_index=[2, 5802], test_pos_edge_index=[2, 11605], train_pos_edge_index=[2, 197290], train_neg_adj_mask=[2079, 2079], val_neg_edge_index=[2, 5802], test_neg_edge_index=[2, 11605])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import to_networkx, from_networkx, train_test_split_edges\n",
    "G7_torch = from_networkx(G7)\n",
    "print(G7_torch)\n",
    "G7_torch = train_test_split_edges(G7_torch)\n",
    "print(G7_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 486,  104, 2004,  ..., 1193,  556,   87],\n",
      "        [1985, 1600, 2054,  ..., 1631,  631,   99]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(G7_torch.val_neg_edge_index)\n",
    "[x for x in G7.edges() if x == (970, 2068)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_index = set(np.array(torch.unique(G7_torch.train_pos_edge_index)))\n",
    "nodes = set([x for x in G7.nodes()])\n",
    "test_nodes = list(nodes - training_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "197290 + 5802 + 11605 + 11605 + 5802 == 232104"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se genera una red de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = GCNConv(np.shape(G7_torch.x_encoded)[1], 32)\n",
    "        self.conv2 = GCNConv(32, 16)\n",
    "\n",
    "    def encode(self):\n",
    "        x = self.conv1(G7_torch.x_encoded, G7_torch.train_pos_edge_index) # convolution 1\n",
    "        x = x.relu()\n",
    "        return self.conv2(x, G7_torch.train_pos_edge_index) # convolution 2\n",
    "        \n",
    "    def decode(self, z, pos_edge_index, neg_edge_index): # only pos and neg edges\n",
    "        edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=-1) # concatenate pos and neg edges\n",
    "        logits = (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)  # dot product \n",
    "        return logits\n",
    "\n",
    "    def decode_all(self, z): \n",
    "        prob_adj = z @ z.t() # get adj NxN\n",
    "        return (prob_adj > 0).nonzero(as_tuple=False).t() # get predicted edge_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = Net(), G7_torch\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Se generan las funciones de entrenamiento y de testeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_link_labels(pos_edge_index, neg_edge_index):\n",
    "    # returns a tensor:\n",
    "    # [1,1,1,1,...,0,0,0,0,0,..] with the number of ones is equal to the lenght of pos_edge_index\n",
    "    # and the number of zeros is equal to the length of neg_edge_index\n",
    "    E = pos_edge_index.size(1) + neg_edge_index.size(1)\n",
    "    link_labels = torch.zeros(E, dtype=torch.float)\n",
    "    link_labels[:pos_edge_index.size(1)] = 1.\n",
    "    return link_labels\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.train_pos_edge_index, #positive edges\n",
    "        num_nodes=data.num_nodes, # number of nodes\n",
    "        num_neg_samples=data.train_pos_edge_index.size(1)) # number of neg_sample equal to number of pos_edges\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model.encode() #encode\n",
    "    link_logits = model.decode(z, data.train_pos_edge_index, neg_edge_index) # decode\n",
    "    \n",
    "    link_labels = get_link_labels(data.train_pos_edge_index, neg_edge_index)\n",
    "    loss = F.binary_cross_entropy_with_logits(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    perfs = []\n",
    "    for prefix in [\"val\", \"test\"]:\n",
    "        pos_edge_index = data[f'{prefix}_pos_edge_index']\n",
    "        neg_edge_index = data[f'{prefix}_neg_edge_index']\n",
    "\n",
    "        z = model.encode() # encode train\n",
    "        link_logits = model.decode(z, pos_edge_index, neg_edge_index) # decode test or val\n",
    "        link_probs = link_logits.sigmoid() # apply sigmoid\n",
    "        \n",
    "        link_labels = get_link_labels(pos_edge_index, neg_edge_index) # get link\n",
    "\n",
    "        perfs.append(roc_auc_score(link_labels.cpu(), link_probs.cpu())) #compute roc_auc score\n",
    "    return perfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss: 0.4580, Val: 0.9879, Test: 0.9880\n",
      "Epoch: 020, Loss: 0.4111, Val: 0.9879, Test: 0.9880\n",
      "Epoch: 030, Loss: 0.4025, Val: 0.9879, Test: 0.9880\n",
      "Epoch: 040, Loss: 0.4008, Val: 0.9879, Test: 0.9880\n",
      "Epoch: 050, Loss: 0.3996, Val: 0.9879, Test: 0.9880\n",
      "Epoch: 060, Loss: 0.4014, Val: 0.9879, Test: 0.9880\n"
     ]
    }
   ],
   "source": [
    "best_val_perf = test_perf = 0\n",
    "for epoch in range(1, 61):\n",
    "    train_loss = train()\n",
    "    val_perf, tmp_test_perf = test()\n",
    "    if val_perf > best_val_perf:\n",
    "        best_val_perf = val_perf\n",
    "        test_perf = tmp_test_perf\n",
    "    log = 'Epoch: {:03d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'\n",
    "    if epoch % 10 == 0:\n",
    "        print(log.format(epoch, train_loss, best_val_perf, test_perf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = model.encode()\n",
    "final_edge_index = model.decode_all(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2079, 16]) torch.Size([2, 4157579])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0121, -0.2469,  0.1043,  ..., -0.0589,  0.1427, -0.0432],\n",
       "         [-0.0224, -0.0668, -0.0039,  ...,  0.0736,  0.1232,  0.1165],\n",
       "         [ 0.2553, -0.3356,  0.2258,  ..., -0.2967, -0.1108,  0.0763],\n",
       "         ...,\n",
       "         [ 0.0214, -0.0549,  0.2124,  ..., -0.2616, -0.3166, -0.1025],\n",
       "         [-0.0227, -0.0676, -0.0037,  ...,  0.0742,  0.1243,  0.1176],\n",
       "         [-0.1550, -0.2558,  0.0857,  ...,  0.1298, -0.0690, -0.2096]],\n",
       "        grad_fn=<AddBackward0>),\n",
       " tensor([[   0,    0,    0,  ..., 2078, 2078, 2078],\n",
       "         [   0,    1,    2,  ..., 2076, 2077, 2078]]))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(np.shape(z),np.shape(final_edge_index))\n",
    "z, final_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_list = (np.array(final_edge_index)).tolist()\n",
    "links_finales = np.array([x for x in prediction_list if x[0] in (test_nodes) ])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8eb729d9847d079dc3a83fb04234ab0b883f85e4ec6aa38230e8b7f84fe4a950"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('reto10_rojo': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
